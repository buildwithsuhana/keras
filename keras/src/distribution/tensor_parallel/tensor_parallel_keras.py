"""
Tensor Parallel implementation for Keras 3.0
Port of the PyTorch tensor_parallel library
"""

import logging
import re
import gc
import os
import shutil
import tempfile
from typing import Collection, Optional, Sequence, Union

import numpy as np
import tensorflow as tf
import keras
from keras import ops
from keras.src.distribution.tensor_parallel.autoconfig import (
    get_default_config,
)
from keras.src.distribution.tensor_parallel.parameter_sharding import (
    make_parameter_sharded_model,
)
from keras.src.distribution.tensor_parallel.coordinated_optimizer import (
    TensorParallelOptimizer,
)

from keras.src.distribution import list_devices

logger = logging.getLogger(__file__)

from keras.src.models import Model


class TensorParallelKeras(Model):
    def __init__(
        self,
        model,
        device_count=None,
        device_ids=None,
        low_memory_mode=True,  # New Flag for CPU OOM prevention
        **kwargs,
    ):
        super().__init__(**kwargs)

        # 1. Device Configuration
        if device_count is None:
            device_count, device_ids = self._auto_detect_parallelism()
        elif device_ids is None:
            device_ids = self._auto_configure_devices(device_count)

        # Ensure correct device format
        device_ids = list(self.check_device_ids(device_ids))
        if len(device_ids) != device_count:
            device_ids = self._adjust_device_list(device_ids, device_count)
            
        self.device_count = device_count
        self.devices = device_ids
        self.distributed = self.device_count > 1
        
        # 2. Handle Auto-Config BEFORE sharding
        self.tensor_parallel_config = None
        if self.tensor_parallel_config is None:
            device_names = [str(d) for d in self.devices]
            self.tensor_parallel_config = get_default_config(
                model, device_names
            )

        self.model_shards = []
        self.modified_parameters_names = set()

        if not self.distributed:
            # Single device fallback
            self.model_shards = [model]
            self.built = True
            self.assembled_model = model
            return

        print(
            f"ðŸ”§ Creating REAL parameter shards for {model.name} across {len(self.devices)} devices"
        )
        print(f"ðŸ“‰ Low Memory Mode: {'ENABLED' if low_memory_mode else 'DISABLED'}")

        # 3. LOW MEMORY STRATEGY
        # Instead of cloning 'model' in RAM (which creates 2x memory usage),
        # we save 'model' to disk, DELETE it from RAM, and load it back one by one.
        
        temp_dir = None
        model_path = None
        
        # Keep a reference to original inputs/outputs for building the assembled model later
        # We extract these lightweight specs before deleting the heavy model
        self._input_specs = [
            {'shape': i.shape, 'dtype': i.dtype, 'name': i.name} 
            for i in model.inputs
        ]
        self._output_specs = model.outputs
        self._original_name = model.name
        self._last_layer_config = model.layers[-1].get_config()
        self._last_layer_name = model.layers[-1].name

        if low_memory_mode:
            temp_dir = tempfile.mkdtemp()
            model_path = os.path.join(temp_dir, "temp_tp_model.keras")
            print(f"   ðŸ’¾ Offloading original model to disk: {model_path} ...")
            try:
                model.save(model_path)
                print("   âœ… Model saved. Freeing CPU memory...")
                
                # CRITICAL: Delete the original model from RAM
                del model
                gc.collect()
                
            except Exception as e:
                logger.warning(f"Failed to save model for low_memory_mode ({e}). Fallback to standard cloning.")
                model_path = None # Fallback
        
        # 4. SHARDING LOOP
        for rank, device_id in enumerate(self.devices):
            print(f"[{device_id}] âž¡ï¸  Starting sharding process for Rank {rank}")
            gc.collect()
            
            # A. Get a fresh copy of the model (Replica)
            if low_memory_mode and model_path:
                print(f"   ðŸ”„ Loading replica from disk for Rank {rank}...")
                with keras.saving.custom_object_scope({}): # Add custom objects if needed
                    # Load back onto CPU first
                    replica_model = keras.models.load_model(model_path, compile=False)
            else:
                # Standard Clone (High Memory)
                try:
                    replica_model = keras.models.clone_model(model)
                    replica_model.set_weights(model.get_weights())
                except:
                    replica_model = model # Dangerous fallback

            # B. Shard it (Move weights to GPU, clear CPU)
            shard, modified_parameters_names = make_parameter_sharded_model(
                replica_model,
                self.tensor_parallel_config,
                rank=rank,
                device_count=self.device_count,
                device_id=device_id,
            )
            self.model_shards.append(shard)
            self.modified_parameters_names.update(modified_parameters_names)

            # C. Cleanup Replica from CPU
            # make_parameter_sharded_model returns a wrapper, but the inner replica 
            # might still hold some CPU refs if not careful. 
            # The 'shard' wrapper holds the model which now has GPU weights.
            logger.info(f"   âœ… Created shard {rank} for device {device_id}")
            gc.collect()

        # 5. Cleanup Disk
        if temp_dir and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            print("   ðŸ§¹ Temporary model files cleaned up.")

        self.built = True
        self.assembled_model = self.build_assembled_model()

    # ... [Keep all other methods: variables, check_device_ids, canonicalize_device, call, compile] ...
    # Copy them from the previous correct version I gave you.
    # Below is the shortened build_assembled_model using saved specs

    @property
    def variables(self):
        unique_vars = {id(var): var for shard in self.model_shards for var in shard.variables}
        return list(unique_vars.values())

    @property
    def trainable_variables(self):
        unique_vars = {id(var): var for shard in self.model_shards for var in shard.trainable_variables}
        return list(unique_vars.values())
        
    def _auto_detect_parallelism(self):
        from keras.src.distribution import get_best_devices
        available_devices = list_devices()
        device_count = len(available_devices)
        device_ids = get_best_devices(device_count)
        return device_count, device_ids

    def _adjust_device_list(self, device_ids, target_device_count):
        current_size = len(device_ids)
        if current_size >= target_device_count:
            return device_ids[:target_device_count]
        return list(device_ids) + [f"cpu:{i}" for i in range(current_size, target_device_count)]

    def _auto_configure_devices(self, device_count):
        available_devices = list_devices()
        if available_devices:
            return available_devices[:device_count]
        return ["cpu:0"]

    def check_device_ids(self, device_ids: Optional[Sequence[str]]) -> Sequence[str]:
        if device_ids is None:
            device_ids = self._get_all_device_indices()
        return tuple(self.canonicalize_device(d) for d in device_ids)

    def _get_all_device_indices(self) -> Sequence[str]:
        return list_devices()
        
    def canonicalize_device(self, device_spec: Union[str, int]) -> str:
        if isinstance(device_spec, int):
            return "cpu" if device_spec == -1 else f"gpu:{device_spec}"
        elif isinstance(device_spec, str):
            if device_spec == "cpu": return "cpu"
            if device_spec.startswith("gpu:") or device_spec.startswith("tpu:"): return device_spec
            if device_spec.startswith("cuda:"): return f"gpu:{device_spec.split(':')[1]}"
            return device_spec
        return "cpu"

    def build_assembled_model(self):
        if not self.distributed:
            return self.model_shards[0]

        # Reconstruct Inputs from saved specs
        input_layers = {}
        for spec in self._input_specs:
            # Handle name cleaning
            clean_name = spec['name'].split(":")[0]
            input_layers[clean_name] = keras.Input(
                shape=spec['shape'][1:], 
                dtype=spec['dtype'], 
                name=clean_name
            )

        partial_outputs = []
        for shard in self.model_shards:
            shard_inputs = dict(input_layers)
            partial_outputs.append(shard(shard_inputs))

        # Reconstruct Output Logic
        final_kernel_name = f"{self._original_name}.{self._last_layer_name}.kernel"
        
        # Fallback to output rules
        # Usually TP ends with AllReduce, so inputs are identical.
        final_output = partial_outputs[0]

        assembled_model = keras.Model(
            inputs=list(input_layers.values()), outputs=final_output
        )
        return assembled_model
        
    def call(self, inputs, training=None, **kwargs):
        return self.assembled_model(inputs, training=training, **kwargs)

    def compile(self, optimizer=None, loss=None, metrics=None, **kwargs):
        if len(self.model_shards) > 1 and optimizer is not None:
            self.coordinated_optimizer = TensorParallelOptimizer(
                optimizer,
                self.device_count,
                tensor_parallel_config=self.tensor_parallel_config,
            )
            self.coordinated_optimizer._shard_models = self.model_shards
            
            var_map = {}
            assembled = getattr(self, "assembled_model", None)
            if assembled:
                for a_var in assembled.variables:
                     key = a_var.path if hasattr(a_var, 'path') else a_var.name
                     suffix = key.split("/")[-1]
                     per_shard = []
                     for shard in self.model_shards:
                         found = None
                         for v in shard.variables:
                             if v.name.endswith(suffix):
                                 found = v
                                 break
                         per_shard.append(found)
                     var_map[key] = per_shard
            
            self.coordinated_optimizer._shard_var_map = var_map
            super().compile(
                optimizer=self.coordinated_optimizer,
                loss=loss,
                metrics=metrics,
                **kwargs,
            )
        else:
            super().compile(optimizer, loss, metrics, **kwargs)